{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 1:\n",
    "Utilice expresiones regulares para validar las siguintes situaciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Expresión Regular para Correos Electrónicos \n",
    "Implemente una regex para validar un correo electrónico en general, a continuación se muestran algunos ejemplos.\n",
    "\n",
    "- Guate.360-porelmundo@miguate.com \n",
    "- Miercoles3@hotmail.com \n",
    "- Progra3.galileo@galileo.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_REGEX = r\"([a-zA-Z0-9._%-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})\"\n",
    "\n",
    "email_examples = [ \"Guate.360-porelmundo@miguate.com\", \n",
    "\"Miercoles3@hotmail.com\", \n",
    "\"Progra3.galileo@galileo.edu\", \"novalid.email.com.gt\"]\n",
    "\n",
    "def isAnEmail(email):\n",
    "    return re.match(EMAIL_REGEX, email) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for email in email_examples:\n",
    "    print(f'{email} is an email: {isAnEmail(email)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Expresión regular para Network URL\n",
    "\n",
    "Implemente una regex para validar la dirección url de una página web con los tipos de domino (.com, .org, .edu). Note que la url incluye el protocolo (http o https) y los símbolos (//www.), a continuacióon se muestran algunos ejemplos: \n",
    "\n",
    "- https://www.guate360-porelmundo.com \n",
    "- http://www.a2.net\n",
    "- https://www.galileo.edu \n",
    "- http://www.8.org (No valida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_REGEX = r\"(https?)://(www\\.)?(([a-zA-Z])([a-zA-Z0-9-])*)(.[a-zA-Z]{2,})+\"\n",
    "\n",
    "url_examples = [\n",
    "\"https://www.guate360-porelmundo.com\",\n",
    "\"http://www.a2.net\" ,\n",
    "\"https://www.galileo.edu.gt\",\n",
    "\"http://www.8.org \"\n",
    "]\n",
    "\n",
    "def isAnURL(url):\n",
    "    return re.match(URL_REGEX, url) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in url_examples:\n",
    "    print(f'{url} is an URL: {isAnURL(url)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Expresión Regular para una MAC Address\n",
    "\n",
    "Implemente una regex para validar una MAC Address, notar que las mac addres están divididas en 6 bloques de caracteres hexadecimales, es decir que los símbolos solo pueden variar del 0 al 9 y las letras de la A a la F. a continuación se muestran algunos ejemplos:\n",
    "\n",
    "- 5A 6F AF 8C 9B 1D\n",
    "- 6D 6C 4D 3A EB 3F\n",
    "- 3A 7C FA C8 6D 4J (no valida por que el ultimo bloque contiene una J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAC_ADDRESS_REGEX = r\"([0-9A-Fa-f]{2}\\ ){5}([[0-9A-Fa-f]{2})\"\n",
    "\n",
    "mac_address_examples = [\"5A 6F AF 8C 9B 1D\", \"6D 6C 4D 3A EB 3F\", \"3A 7C FA C8 6D 4J\"]\n",
    "\n",
    "def isAMacAddress(mac_address) :\n",
    "    return re.match(MAC_ADDRESS_REGEX, mac_address) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mac_address in mac_address_examples:\n",
    "    print(f'{mac_address} is an Mac Address: {isAMacAddress(mac_address)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Expresión regular para una dirección IPv4\n",
    "\n",
    "Implemente una regex para validar una dirección IPv4, notar que las direcciones IPv4 están divididas en 4 bloques de valores los cuales solo pueden ir desde 0 hasta 255, una ip donde algunos de sus bloques sea mayor a 255 no es valida, además tome en cuenta que cada bloque estáa separada por un punto. A continuación se muestran algunos ejemplos:\n",
    "\n",
    "- 192.16.8.1\n",
    "- 234.56.78.90\n",
    "- 1.2.3.4\n",
    "- 192.168.45.345 (no valida por que el ultimo bloque es mayor a 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP_REGEX = r\"((25[0-5]|(2[0-4]|1\\d|[1-9]|)\\d)\\.?\\b){4}\"\n",
    "\n",
    "ip_examples = [\"192.16.8.1\", \"234.56.78.90\", \"1.2.3.4\",\"192.168.45.345\"]\n",
    "\n",
    "def isAnIPv4(ip) :\n",
    "    return re.match(IP_REGEX, ip) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ip in ip_examples:\n",
    "    print(f'{ip} is an IP Address: {isAnIPv4(ip)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Expresión regular para fecha\n",
    "Implemente una regex para validar una fecha con la secuencia día-mes-año donde el día, mes y año puedan estar separados ya sea por el caracter / o el caracter - o el caracter ., notar que las fechas son validas si los si los días están definidos desde el 1 al 31, el mes del 1 al 12 y el año de 2000 al 2019. También debe tomar en cuenta que los días y meses pueden estar escritos ya sea con uno o dos caracteres por ejemplo: Enero puede escribirse como 1 o como 01. Los añs también pueden expresarse ya sea con dos o con cuatro caracteres por ejemplo: 19 o 2019 son validos. A continuación se muestran algunos ejemplos:\n",
    "\n",
    "- 20/1/2019\n",
    "- 12.03.2005\n",
    "- 31-11-08\n",
    "- 1-1-2012\n",
    "- 12-12-22 (no valida, por que el an ̃o supera al 2019).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE_REGEX = r\"^(0?[1-9]|[1-2][0-9]|3[0-1])([/.-])(0?[1-9]|1[0-2])([/.-])((20)?[0-1][0-9]\\b)\"\n",
    "\n",
    "date_examples = [\"20/1/2019\", \n",
    "\"12.03.2005\",\n",
    "\"31-11-08\",\n",
    "\"1-1-2012\",\n",
    "\"12-12-22\",\n",
    "\"30-12-2019\",\n",
    "\"0-1-1\"]\n",
    "\n",
    "\n",
    "def isADate(date) :\n",
    "    return re.match(DATE_REGEX, date) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in date_examples:\n",
    "    print(f'{date} is an Date: {isADate(date)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 2\n",
    "\n",
    "En la carpeta encontrará adjuntos 21 documentos que tiene 100 fechas en la secuencia dias-mes-año pero con distinto separador y distinto formato de mes, en algunos casos aparece un numero y en otros el nombre del mes en ingles, por ejemplo: Enero pueder aparecer como 1 o como Jan. Utilice Python y expresiones regulares para encontrar el día, mes y año promedio total del los 21 archivos, los resultados deben ser un double.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "DATE_REGEX = r\"^(0?[1-9]|[1-2][0-9]|3[0-1])([/.-])(0?[1-9]|1[0-2]|[a-zA-Z]{3})([/.-])(20[0-9][0-9]\\b)\"\n",
    "\n",
    "MONTHS = {\n",
    "    'Jan' : 1, 'Feb' : 2, 'Mar' : 3, 'Apr' :4, 'May' : 5, 'Jun' : 6, \n",
    "    'Jul' : 7, 'Aug' : 8, \"Sep\" : 9, 'Oct' :10, 'Nov' : 11, 'Dec' :12\n",
    "     \n",
    "}\n",
    "\n",
    "def split_date(date):\n",
    "    date_match = re.match(DATE_REGEX, date)\n",
    "\n",
    "    if (date_match is None) : \n",
    "        print(f'{date} is not date')\n",
    "        return [0, 0, 0]\n",
    "    \n",
    "    day = int(date_match.groups()[0])\n",
    "    month = date_match.groups()[2]\n",
    "    year = int(date_match.groups()[4])\n",
    "\n",
    "    month_number_match = re.match(r\"0?[1-9]|1[0-2]\", month)\n",
    "\n",
    "    if (month_number_match is not None) :\n",
    "        month_number = int(month)\n",
    "    else : \n",
    "        month_number = MONTHS[month]\n",
    "\n",
    "    return [day,month_number,year]\n",
    "\n",
    "\n",
    "file_names = os.listdir('./Datos')\n",
    "file_names.sort()\n",
    "dates = []\n",
    "\n",
    "for file_name in file_names:\n",
    "    file = open(f'./Datos/{file_name}', mode='r' , encoding='utf-8-sig')\n",
    "\n",
    "    for line in file.readlines():\n",
    "        dates.append(split_date(f'{line.strip()}'))\n",
    "\n",
    "dataset =  pd.DataFrame(dates, columns=['day', 'month', 'year'])\n",
    "\n",
    "print(\"Promedio del día:\", dataset['day'].mean())\n",
    "print(\"Promedio del mes:\", dataset['month'].mean())\n",
    "print(\"Promedio del año:\", dataset['year'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 3 \n",
    "\n",
    "Descargue el Dataset (de click aqu ́ı para descargar) el cual contiene aproximadamente 800,000 tweets de diversos temas.\n",
    "\n",
    "Usando CoLab y expresiones regulares. Determine los 3 usuarios más populares dentro del dataset. Luego arme un corpus el cual contenga los siguientes elementos por cada usuario seleccio- nado:\n",
    "\n",
    "- Content: Tweet.\n",
    "- Metadata: ID, Timestamp, Length (este valor hay que calcularlo).\n",
    "\n",
    "\n",
    "Posterior a tener sus 3 corpus creados, responda: ¿Razón por la que citan a ese usuario? para esto es necesario que extraiga el contexto de cada tweet y verifique cuales son las palabras que más rodean al nombre de usuario. Para extraer un contexto valido y debido a la naturaleza del tipo de datos que est ́an disponibles en nuestro dataset le recomendamos seguir los siguientes pasos:\n",
    "\n",
    "1. Remover stopwords.\n",
    "2. Realizar stemming y lemmatizaci ́on.\n",
    "3. Mostrar un wordcloud con el top 10 para cada usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "USER_REGULAR_EXPRESSION = r\"(@)(\\w){1,15}\"\n",
    "\n",
    "def get_mentions(tweet) :\n",
    "    user_match = re.finditer(USER_REGULAR_EXPRESSION, tweet)\n",
    "    return [x.group() for x in  user_match]\n",
    "\n",
    "dataset = pd.read_csv('tw_source.csv', header=None)\n",
    "dataset.columns = ['_', 'id', 'date', 'type', 'user', 'tweet']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>type</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>length</th>\n",
       "      <th>mention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>115</td>\n",
       "      <td>@switchfoot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>111</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>89</td>\n",
       "      <td>@Kenichan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>47</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>111</td>\n",
       "      <td>@nationwideclass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648110</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>57</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648111</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>65</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648112</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>62</td>\n",
       "      <td>@SpeakingUpH4H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648113</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>62</td>\n",
       "      <td>@SpeakingUpH4H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648114</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>62</td>\n",
       "      <td>@SpeakingUpH4H</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1648115 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         _          id                          date      type   \n",
       "0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
       "1        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...     ..         ...                           ...       ...   \n",
       "1648110  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1648111  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1648112  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "1648113  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "1648114  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                              tweet   \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \\\n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "...                  ...                                                ...   \n",
       "1648110           bpbabe  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1648111     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1648112   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "1648113   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "1648114   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "         length           mention  \n",
       "0           115       @switchfoot  \n",
       "1           111              None  \n",
       "2            89         @Kenichan  \n",
       "3            47              None  \n",
       "4           111  @nationwideclass  \n",
       "...         ...               ...  \n",
       "1648110      57              None  \n",
       "1648111      65              None  \n",
       "1648112      62    @SpeakingUpH4H  \n",
       "1648113      62    @SpeakingUpH4H  \n",
       "1648114      62    @SpeakingUpH4H  \n",
       "\n",
       "[1648115 rows x 8 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['length'] = 0\n",
    "dataset['mention'] = None\n",
    "\n",
    "temp_df = []\n",
    "index = 0\n",
    "\n",
    "for row in dataset.itertuples(index=False):\n",
    "    mentions = get_mentions(row.tweet)\n",
    "    row_ = list(row)\n",
    "    row_[len(row_) - 2] = len(row.tweet)\n",
    "    if len(mentions) == 0 :\n",
    "        temp_df.append(row_)\n",
    "    else:\n",
    "        for mention in mentions:\n",
    "            row_[len(row_) - 1] = mention\n",
    "            temp_df.append(row_)     \n",
    "\n",
    "    index += 1 \n",
    "\n",
    "\n",
    "tweet_dataset = pd.DataFrame(temp_df, columns=dataset.columns)\n",
    "tweet_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine los 3 usuarios más populares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mention\n",
       "@mileycyrus    4472\n",
       "@tommcfly      3821\n",
       "@ddlovato      3451\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions = tweet_dataset['mention'].value_counts()[:3]\n",
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "miley_cyrus_mentions = tweet_dataset[tweet_dataset['mention'] == '@mileycyrus']\n",
    "tommcfly_mentions = tweet_dataset[tweet_dataset['mention'] == '@tommcfly']\n",
    "ddlovato_mentions = tweet_dataset[tweet_dataset['mention'] == '@ddlovato']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "STEMMER = PorterStemmer()\n",
    "LEMMATIZER =  WordNetLemmatizer()\n",
    "\n",
    "def clean_tweet(tweet, user):\n",
    "    PUNCTUATION_REGEX = r\"[^\\w\\s]\"\n",
    "    clean_tweet = tweet.replace(user, '')\n",
    "    return re.sub(PUNCTUATION_REGEX, '', clean_tweet.strip())\n",
    "\n",
    "def get_word_tokens(tweet):\n",
    "    return word_tokenize(tweet)\n",
    "\n",
    "def remove_stop_words(word_tokens) :\n",
    "     \n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "    filtered_sentence = []\n",
    "    \n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    \n",
    "    return filtered_sentence\n",
    "\n",
    "def stem (words) :\n",
    "    stem_words = []\n",
    "    for word in words:\n",
    "        stem_words.append(STEMMER.stem(word))\n",
    "    return stem_words\n",
    "\n",
    "def lemmatize(words):\n",
    "    lemmatize_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        lemmatize_words.append(LEMMATIZER.lemmatize(word))\n",
    "    return lemmatize_words\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet, user):\n",
    "    tweet_clean = clean_tweet(tweet, user)\n",
    "    word_tokens = get_word_tokens(tweet_clean)\n",
    "    words_without_stopwords = remove_stop_words(word_tokens)\n",
    "    return lemmatize(words_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 de palabras para Miley Cyrus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I        1218\n",
       "love      709\n",
       "u         632\n",
       "miley     426\n",
       "im        339\n",
       "Miley     335\n",
       "cant      287\n",
       "dont      260\n",
       "like      249\n",
       "good      245\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_from_tweet = []\n",
    "for row in miley_cyrus_mentions.itertuples(index=False):\n",
    "    words_from_tweet.extend(list(preprocess_tweet(row.tweet, row.mention)))\n",
    "\n",
    "words_dataset = pd.DataFrame(words_from_tweet)\n",
    "words_dataset.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 de palabras para Tom Mcfly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I         892\n",
       "u         417\n",
       "love      354\n",
       "please    342\n",
       "tom       320\n",
       "Tom       318\n",
       "say       286\n",
       "guy       273\n",
       "x         250\n",
       "im        248\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_from_tweet = []\n",
    "for row in tommcfly_mentions.itertuples(index=False):\n",
    "    words_from_tweet.extend(list(preprocess_tweet(row.tweet, row.mention)))\n",
    "\n",
    "words_dataset = pd.DataFrame(words_from_tweet)\n",
    "words_dataset.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 de palabras para Demi Lovato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31422, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "I       1215\n",
       "love     494\n",
       "u        468\n",
       "demi     313\n",
       "im       291\n",
       "cant     267\n",
       "come     255\n",
       "see      237\n",
       "like     221\n",
       "hope     220\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_from_tweet = []\n",
    "for row in ddlovato_mentions.itertuples(index=False):\n",
    "    words_from_tweet.extend(list(preprocess_tweet(row.tweet, row.mention)))\n",
    "\n",
    "words_dataset = pd.DataFrame(words_from_tweet)\n",
    "print(words_dataset.shape)\n",
    "words_dataset.value_counts()[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
